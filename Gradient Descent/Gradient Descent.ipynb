{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb4442fe",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a441a1cc",
   "metadata": {},
   "source": [
    "\n",
    "### The Mean Squared Error (MSE) loss function is defined as:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^2 $$\n",
    "\n",
    "Where:\n",
    "- $m$ is the number of training examples,\n",
    "- $h_{\\theta}(x^{(i)})$ is the prediction for the $i^{th}$ example,\n",
    "- $y^{(i)}$ is the true value for the $i^{th}$ example,\n",
    "- $\\theta$ is the parameter vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474004c0",
   "metadata": {},
   "source": [
    "A typical Machine Learning setup has\n",
    "\n",
    "<ul>\n",
    "    <li> Data</li>\n",
    "    <li> Model</li>\n",
    "    <li> Parameters / Inputs / features</li>\n",
    "    <li> Learning algorithm</li>\n",
    "    <li> Objective Function / Error / Loss</li>\n",
    "</ul>\n",
    "\n",
    "The point here is how do you define a princpled way for learning rate? \n",
    "\n",
    "To rescue comes Gradient Descent. \n",
    "\n",
    "### Formula\n",
    "The update rule of the Gradient Descent algorithm is:\n",
    "\n",
    "$$ \\theta = \\theta - \\alpha \\cdot \\nabla_{\\theta} J(\\theta) $$\n",
    "\n",
    "Where:\n",
    "- $\\theta$ is the parameter vector\n",
    "- $\\alpha$ is the learning rate\n",
    "- $\\nabla_{\\theta} J(\\theta)$ is the gradient of the loss function $J$ with respect to $\\theta$\n",
    "\n",
    "\n",
    "### partial derivatives of gradient Descent\n",
    "\n",
    "For the simple linear regression model defined by $y = mx + b$, the cost function $J(m, b)$ using mean squared error is:\n",
    "\n",
    "$$ J(m, b) = \\frac{1}{N} \\sum_{i=1}^{N} (y^{(i)} - (mx^{(i)} + b))^2 $$\n",
    "\n",
    "The partial derivatives of the cost function with respect to $m$ and $b$ are:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial m} = -\\frac{2}{N} \\sum_{i=1}^{N} x^{(i)}(y^{(i)} - (mx^{(i)} + b)) $$\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial b} = -\\frac{2}{N} \\sum_{i=1}^{N} (y^{(i)} - (mx^{(i)} + b)) $$\n",
    "\n",
    "Where:\n",
    "- $N$ is the number of observations in the data\n",
    "- $x^{(i)}$ and $y^{(i)}$ are the input and output of the $i^{th}$ observation respectively\n",
    "\n",
    "\n",
    "\n",
    "<b>Note:  We use Sigmoid family functions as the basis standard for Learning algorithm </b>\n",
    "<b> proof for the Gradient descent can be seen at <a href =\"https://www.youtube.com/watch?v=giZD8yzXEZ4&list=PLEAYkSg4uSQ1r-2XrJ_GBzzS6I-f8yfRU&index=22\" target=\"_blank\"> YouTube Video </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa098f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e47b793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a536b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1,2,3,4,5])\n",
    "Y = np.array([5,7,9,11,13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff59db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x,y):\n",
    "    m_curr = b_curr = 0\n",
    "    max_epochs = 1000\n",
    "    learning_rate = 0.001 ## it is arbitrary we change it based on algorithm behaviour\n",
    "    n= len(x)\n",
    "    \n",
    "    for i in range(max_epochs):\n",
    "        y_pred = m_curr * x + b_curr\n",
    "        \n",
    "        md = -(2/n) * sum(x*(y-y_pred))\n",
    "        bd = -(2/n) * sum((y-y_pred))\n",
    "        \n",
    "        # why we have to do a negative and not add is also clearly explained in the Link\n",
    "        # As a standard we always move away from gradient descent\n",
    "        m_curr = m - learning_rate * md\n",
    "        b_curr = b_curr - learning_rate * bd\n",
    "        \n",
    "        print(\"m {}, b {}, max_epochs {}\".format(m_curr,b_curr))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944bf90b",
   "metadata": {},
   "source": [
    "### How do you evaluate whether your learning is good ?\n",
    "\n",
    "we calculate cost function or loss function at every iteration if it is decreasing we are going in correct direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb18001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x,y):\n",
    "    m_curr = b_curr = 0\n",
    "    max_epochs = 10\n",
    "    learning_rate = 0.001 ## it is arbitrary we change it based on algorithm behaviour\n",
    "    n= len(x)\n",
    "    \n",
    "    for i in range(max_epochs):\n",
    "        y_pred = m_curr * x + b_curr\n",
    "        cost = -(1/n) * sum([val**2 for val in (y-y_pred)])\n",
    "        \n",
    "        md = -(2/n) * sum(x*(y-y_pred))\n",
    "        bd = -(2/n) * sum((y-y_pred))\n",
    "        \n",
    "        # why we have to do a negative and not add is also clearly explained in the Link\n",
    "        # As a standard we always move away from gradient descent\n",
    "        m_curr = m - learning_rate * md\n",
    "        b_curr = b_curr - learning_rate * bd\n",
    "        \n",
    "        print(\"m {}, b {},cost {}, max_epochs {}\".format(m_curr,b_curr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
